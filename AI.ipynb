{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\AITes\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import math\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from keras.layers import Dense, LSTM, Dropout\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.models import Sequential, load_model\n",
    "import statistics as s\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "sys.path.append('src')\n",
    "import logging\n",
    "from colorlog import ColoredFormatter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "formatter = ColoredFormatter(\n",
    "    \"[%(asctime)s] [%(log_color)s%(levelname)-s%(reset)s] \\\"%(message)s\\\"\",\n",
    "    datefmt='%Y-%m-%d %H:%M:%S',\n",
    "    log_colors={\n",
    "        'DEBUG': 'cyan',\n",
    "        'INFO': 'green',\n",
    "        'WARNING': 'yellow',\n",
    "        'ERROR': 'red',\n",
    "        'CRITICAL': 'red,bg_white',\n",
    "    }\n",
    ")\n",
    "\n",
    "handler = logging.StreamHandler()\n",
    "handler.setFormatter(formatter)\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.addHandler(handler)\n",
    "logger.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    class Postgres:\n",
    "        user = os.getenv(\"PostgresUser\")\n",
    "        database = os.getenv(\"PostgresDatabase\")\n",
    "        password = os.getenv(\"PostgresPassword\")\n",
    "        port = os.getenv(\"PostgresPort\")\n",
    "        host = os.getenv(\"PostgresHost\")\n",
    "        schema = os.getenv(\"PostgresSchema\")\n",
    "\n",
    "    PORT = os.getenv(\"PORT\")\n",
    "    HOST = os.getenv(\"HOST\")\n",
    "    IS_DEV = True\n",
    "    URL_API= os.getenv(\"URL_API\") \n",
    "\n",
    "    PREDICT_TOKEN = os.getenv(\"PREDICT_TOKEN\")\n",
    "    TRAIN_TOKEN = os.getenv(\"TRAIN_TOKEN\")\n",
    "\n",
    "    class Training:\n",
    "        WS = 3\n",
    "        EPOCH = 100\n",
    "\n",
    "INTERVALS = ['15m', '1h', '6h', '24h']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Feature:\n",
    "    Inputs = [\n",
    "\n",
    "    \"Id\",\n",
    "    \"HolderCount\",\n",
    "    \"TransactionBuyH1\",\n",
    "    \"TransactionSellH1\",\n",
    "    \"VolumeH1\",\n",
    "    \"TransactionBuyH6\",\n",
    "    \"TransactionSellH6\",\n",
    "    \"VolumeH6\",\n",
    "\n",
    "    ]\n",
    "\n",
    "    class Outputs:\n",
    "\n",
    "        PriceChangeM15 = \"TokenPriceM15\"\n",
    "        PriceChangeH1 = \"TokenPriceH1\"\n",
    "        PriceChangeH4 = \"TokenPriceH4\"\n",
    "        PriceChangeH24 = \"TokenPriceH24\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_process_scaless(data, feature_cols):\n",
    "    data_AI = data[feature_cols].copy()\n",
    "    # dataAI = dataAI[:5000]\n",
    "    data_AI = data_AI[::-1] # Reverse it \n",
    "    original_data_length=len(data_AI)\n",
    "\n",
    "    # Drop the N/As\n",
    "    data_AI.dropna(inplace = True)\n",
    "\n",
    "    # Log the length be/af\n",
    "    record_num = len(data_AI)\n",
    "    logger.info(f'Remainings/Original length (dropNA): {record_num}/{original_data_length} ({record_num*100/original_data_length}%)')\n",
    "    \n",
    "    data_AI_group_by_ID=data_AI.groupby('Id')\n",
    "\n",
    "    list_data=[]\n",
    "    max_length_token_data=data_AI_group_by_ID.size().max()\n",
    "\n",
    "    for group_id, group_df in data_AI_group_by_ID:\n",
    "        list_data.append(group_df[1:])\n",
    "    \n",
    "    for data in list_data:\n",
    "        data.drop(\"Id\", axis=1, inplace=True)\n",
    "\n",
    "    WS = Config.Training.WS\n",
    "    \n",
    "        \n",
    "    data_value_only =[]\n",
    "    for each_data in list_data:\n",
    "        valued_data = each_data.iloc[:][1:].values\n",
    "        data_value_only.append(valued_data)\n",
    "\n",
    "    temp=data_value_only\n",
    "    data_value_only=[]\n",
    "    for each_data in temp:\n",
    "        if len(each_data)>WS:\n",
    "            data_value_only.append(each_data)\n",
    "\n",
    "    feature_count = len(feature_cols) -1\n",
    "\n",
    "    total_dfs = len(list_data)\n",
    "    num_dfs_30_percent = int(0.3 * total_dfs)\n",
    "    training_datas, testing_datas = train_test_split(data_value_only, test_size=num_dfs_30_percent, random_state=42, shuffle=True)\n",
    "\n",
    "    x_train_reshaped, combined_y_train = prepare_training_data(training_datas, WS,max_length_token_data, feature_count)\n",
    "    x_test_reshaped, combined_y_test = prepare_training_data(testing_datas, WS,max_length_token_data, feature_count)\n",
    "    return x_train_reshaped, combined_y_train, x_test_reshaped, combined_y_test, max_length_token_data, feature_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_training_data(training_datas_scaled, WS, max_length_token_data, feature_count):\n",
    "    x_trains = []\n",
    "    y_trains = []\n",
    "\n",
    "    x_target_shape = (max_length_token_data, WS, feature_count - 1)\n",
    "    y_target_shape = (max_length_token_data, 1)\n",
    "\n",
    "    for training_data in training_datas_scaled:\n",
    "        x_train = []\n",
    "        y_train = []\n",
    "        for i in range(WS, len(training_data)):\n",
    "            x_item = training_data[i-WS:i, 0:feature_count-1]\n",
    "            y_item = training_data[i, feature_count-1]\n",
    "\n",
    "            x_train.append(x_item)\n",
    "            y_train.append(y_item) \n",
    "\n",
    "        x_train = np.array(x_train)\n",
    "        y_train = np.array(y_train)\n",
    "\n",
    "        x_trains.append(x_train)\n",
    "        y_trains.append(y_train)\n",
    "\n",
    "    x_trains_padded = []\n",
    "    y_trains_padded = []\n",
    "\n",
    "    for each_x_train in x_trains:\n",
    "        if each_x_train.shape[0] == 0:\n",
    "            x_trains_padded.append(each_x_train)\n",
    "            continue\n",
    "\n",
    "        pad_width = [(0, x_target_shape[0] - each_x_train.shape[0]), (0, 0), (0, 0)]\n",
    "        x_padded_array = np.pad(each_x_train, pad_width, mode='constant')\n",
    "        x_padded_array = x_padded_array.reshape(x_target_shape)\n",
    "        x_trains_padded.append(x_padded_array)\n",
    "\n",
    "    for each_y_train in y_trains:\n",
    "        if each_y_train.shape[0] == 0:\n",
    "            y_trains_padded.append(each_y_train)\n",
    "            continue\n",
    "\n",
    "        padding_length = max_length_token_data - len(each_y_train)\n",
    "        padded_array = np.pad(each_y_train, (0, padding_length), mode='constant')\n",
    "        padded_array = padded_array.reshape(y_target_shape)\n",
    "        y_trains_padded.append(padded_array)\n",
    "\n",
    "    combined_x_train = np.array(x_trains_padded)\n",
    "    combined_y_train = np.array(y_trains_padded)\n",
    "\n",
    "    x_train_reshaped = np.reshape(combined_x_train, (combined_x_train.shape[0], combined_x_train.shape[1], -1))\n",
    "\n",
    "    return x_train_reshaped, combined_y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(\n",
    "        # data: pd.DataFrame,\n",
    "        interval):\n",
    "    interval = '15m'\n",
    "    data = pd.read_csv(fr'src/data/new_data.csv')\n",
    "    try:\n",
    "        # ! Validate\n",
    "        if interval not in INTERVALS:\n",
    "            logger.info(f\"Invalid value for 'interval'. Must be in {INTERVALS}.\")\n",
    "            return False\n",
    "        if data is None:\n",
    "            logger.info(f\"[train.py] No data found.\")  \n",
    "            return False\n",
    "        \n",
    "        logger.info(f\"Train model {interval}\")\n",
    "\n",
    "        # ! Preprocess data start.\n",
    "        # Set output\n",
    "        output = []\n",
    "        if interval == '15m':\n",
    "            output.append(Feature.Outputs.PriceChangeM15)\n",
    "        elif interval == '1h':\n",
    "            output.append(Feature.Outputs.PriceChangeH1)\n",
    "        elif interval == '6h':\n",
    "            output.append(Feature.Outputs.PriceChangeH4)\n",
    "        else:\n",
    "            output.append(Feature.Outputs.PriceChangeH24)\n",
    "        feature_cols = Feature.Inputs + output\n",
    "        \n",
    "        x_train, y_train, x_test, y_test, max_length_token_data, feature_count = data_process_scaless(data, feature_cols)\n",
    "        \n",
    "        # ! Define model\n",
    "        model=define_model(interval, max_length_token_data, Config.Training.WS, feature_count, False)\n",
    "        model.summary()\n",
    "        # ! Define model\n",
    "        model=define_model(interval, max_length_token_data, Config.Training.WS, feature_count, False)\n",
    "        model.summary()\n",
    "\n",
    "        # ! Training model\n",
    "        train_model(model, x_train, y_train, Config.Training.EPOCH)\n",
    "\n",
    "        # ! Save model\n",
    "        save_model(model, interval)\n",
    "\n",
    "        # ! Evaluation\n",
    "        evaluation(model, x_test, y_test)\n",
    "\n",
    "        # ! Upload model for Predict Server\n",
    "        # upload_model(interval)\n",
    "        return True\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(e)\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_model(interval:str, max_length_token_data, WS, feature_count, is_scale:bool=True):\n",
    "    # Load the pre-trained LSTM model\n",
    "    modelPath = f\"src/models/{interval}/model.h5\"\n",
    "\n",
    "    if os.path.isfile(modelPath):\n",
    "        pretrained_model = load_model(modelPath)\n",
    "        # Freeze the layers\n",
    "        for layer in pretrained_model.layers:\n",
    "            layer.trainable = False\n",
    "\n",
    "        # Modify the model\n",
    "        model = Sequential(pretrained_model.layers[:-1])  # Removing the original output layer\n",
    "        model.add(Dense(units = max_length_token_data))  \n",
    "        # Add new output layer\n",
    "\n",
    "        # Compile the new model\n",
    "        model.compile(optimizer = 'adam', loss = 'mean_squared_error')\n",
    "        return model\n",
    "    \n",
    "    else:\n",
    "        # Define LSTM model\n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(LSTM(units = 70, return_sequences = True, input_shape = (max_length_token_data, WS*( feature_count-1))))\n",
    "        model.add(Dropout(0.2))\n",
    "\n",
    "        model.add(LSTM(units = 70, return_sequences = True))\n",
    "        model.add(Dropout(0.2))\n",
    "\n",
    "        model.add(LSTM(units = 70, return_sequences = True))\n",
    "        model.add(Dropout(0.2))\n",
    "\n",
    "        model.add(LSTM(units = 70))\n",
    "        model.add(Dropout(0.2))\n",
    "\n",
    "        model.add(Dense(units = max_length_token_data))\n",
    "\n",
    "        # Compile the model\n",
    "        model.compile(optimizer = 'adam', loss = 'mean_squared_error')\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, x_train, y_train, epoch):\n",
    "    history = model.fit(x_train, y_train, epochs = epoch, batch_size = 32, validation_split=0.2)\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, interval):\n",
    "    save_dir = rf'src/models/{interval}'\n",
    "    os.makedirs(save_dir, exist_ok = True)\n",
    "    save_dir = os.path.join(save_dir, \"model.h5\")\n",
    "    model.save(save_dir)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_model(interval: str):\n",
    "    url = f'{Config.URL_API}/api/upload?interval={interval}'\n",
    "    headers = {'Authorization': f\"Bearer {Config.TRAIN_TOKEN}\"}\n",
    "    file_path = rf'src\\models\\{interval}\\model.h5'\n",
    "    with open(file_path, 'rb') as file:\n",
    "        files = {'file': ('model.h5', file, 'application/octet-stream')}\n",
    "        response = requests.post(url, files = files, headers = headers\n",
    "                                )\n",
    "        if response.status_code == 200 and response.headers['content-type'] == 'application/json':\n",
    "            json_data = response.json()\n",
    "            logger.info(json_data)\n",
    "        else:\n",
    "            logger.error(f\"Error: {response.status_code}, {response.text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-05-02 10:37:38] [\u001b[32mINFO\u001b[0m] \"Train model 15m\"\u001b[0m\n",
      "[2024-05-02 10:37:38] [\u001b[32mINFO\u001b[0m] \"Remainings/Original length (dropNA): 78735/160395 (49.088188534555314%)\"\u001b[0m\n",
      "C:\\Users\\AITes\\AppData\\Local\\Temp\\ipykernel_20348\\1488076621.py:23: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data.drop(\"Id\", axis=1, inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\AITes\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\backend.py:1398: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-05-02 10:37:38] [\u001b[33mWARNING\u001b[0m] \"From C:\\Users\\AITes\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\backend.py:1398: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\"\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\AITes\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-05-02 10:37:40] [\u001b[33mWARNING\u001b[0m] \"From C:\\Users\\AITes\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\"\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_4 (LSTM)               (None, 289, 70)           78680     \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 289, 70)           0         \n",
      "                                                                 \n",
      " lstm_5 (LSTM)               (None, 289, 70)           39480     \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 289, 70)           0         \n",
      "                                                                 \n",
      " lstm_6 (LSTM)               (None, 289, 70)           39480     \n",
      "                                                                 \n",
      " dropout_6 (Dropout)         (None, 289, 70)           0         \n",
      "                                                                 \n",
      " lstm_7 (LSTM)               (None, 70)                39480     \n",
      "                                                                 \n",
      " dropout_7 (Dropout)         (None, 70)                0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 289)               20519     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 217639 (850.15 KB)\n",
      "Trainable params: 20519 (80.15 KB)\n",
      "Non-trainable params: 197120 (770.00 KB)\n",
      "_________________________________________________________________\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_4 (LSTM)               (None, 289, 70)           78680     \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 289, 70)           0         \n",
      "                                                                 \n",
      " lstm_5 (LSTM)               (None, 289, 70)           39480     \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 289, 70)           0         \n",
      "                                                                 \n",
      " lstm_6 (LSTM)               (None, 289, 70)           39480     \n",
      "                                                                 \n",
      " dropout_6 (Dropout)         (None, 289, 70)           0         \n",
      "                                                                 \n",
      " lstm_7 (LSTM)               (None, 70)                39480     \n",
      "                                                                 \n",
      " dropout_7 (Dropout)         (None, 70)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 289)               20519     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 217639 (850.15 KB)\n",
      "Trainable params: 20519 (80.15 KB)\n",
      "Non-trainable params: 197120 (770.00 KB)\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-05-02 10:37:42] [\u001b[31mERROR\u001b[0m] \"in user code:\n",
      "\n",
      "    File \"C:\\Users\\AITes\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\engine\\training.py\", line 1401, in train_function  *\n",
      "        return step_function(self, iterator)\n",
      "    File \"C:\\Users\\AITes\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\engine\\training.py\", line 1384, in step_function  **\n",
      "        outputs = model.distribute_strategy.run(run_step, args=(data,))\n",
      "    File \"C:\\Users\\AITes\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\engine\\training.py\", line 1373, in run_step  **\n",
      "        outputs = model.train_step(data)\n",
      "    File \"C:\\Users\\AITes\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\engine\\training.py\", line 1150, in train_step\n",
      "        y_pred = self(x, training=True)\n",
      "    File \"C:\\Users\\AITes\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 70, in error_handler\n",
      "        raise e.with_traceback(filtered_tb) from None\n",
      "    File \"C:\\Users\\AITes\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\engine\\input_spec.py\", line 298, in assert_input_compatibility\n",
      "        raise ValueError(\n",
      "\n",
      "    ValueError: Input 0 of layer \"sequential_1\" is incompatible with the layer: expected shape=(None, 289, 210), found shape=(None, 289, 21)\n",
      "\"\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def evaluation(model, x_test_reshaped, y_test):\n",
    "\n",
    "    print(x_test_reshaped.shape)\n",
    "    print(y_test.shape) \n",
    "    \n",
    "    model.summary()\n",
    "\n",
    "    # Predict using the trained model\n",
    "    predictions = model.predict(x_test_reshaped)\n",
    " \n",
    "    # Get the number of tokens\n",
    "    num_tokens = x_test_reshaped.shape[0]\n",
    "\n",
    "    RMSE_list=[]\n",
    "    Rsquare_list=[]\n",
    "\n",
    "    # Plot predictions vs. actual values for each token\n",
    "    for token_index in range(num_tokens):\n",
    "        token_real_values=y_test[token_index,:,0]\n",
    "        token_predictions=predictions[token_index,:]\n",
    "\n",
    "\n",
    "\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(token_real_values, label='Actual Values')\n",
    "        plt.plot(token_predictions, label='Predictions', linestyle='--')\n",
    "        plt.xlabel('Time Steps')\n",
    "        plt.ylabel('Output Value')\n",
    "        plt.title(f'Predictions vs. Actual Values for Token {token_index + 1}')\n",
    "        plt.legend()\n",
    "        plt.savefig(os.path.join( rf'src/models/15m/test_result', f'plot_token_{token_index + 1}.png'))\n",
    "\n",
    "        plt.close()\n",
    "        # plt.show()\n",
    "\n",
    "        Rsquare = r2_score(token_real_values, token_predictions)\n",
    "        RMSE = mean_squared_error(token_real_values, token_predictions)\n",
    "\n",
    "        RMSE_list.append(RMSE)\n",
    "        Rsquare_list.append(Rsquare)\n",
    "        \n",
    "\n",
    "    logger.info(f'RMSE(~0): {s.mean(RMSE_list)}')\n",
    "    logger.info(f'Rsquare: {s.mean(Rsquare_list)}')\n",
    "\n",
    "\n",
    "training('')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
